# 分布式

## 相关术语

qps：每秒的请求数 （Nginx：5w qps，Tomcat：200 qps）

集群：镜像

分布式：服务模块化拆分

SOA：面向服务架构

## CAP

1. Consistency 一致性

   分布式系统中，数据在多节点存在副本，当数据发生了修改，那么数据同步无法在修改的<font color=red>瞬间</font>广播到所有副本节点，那么在读的时候就可能发生数据脏读

2. Availbility 可用性

   指的是服务是否可用，范围涵盖终端客户访问我们的系统或者是集群内部相互通讯交换数据

   也就是说在 Client 向 Server 发起请求时，服务器返回了正确的响应，称之为可用，反之为不可用

   1S法则：是面向Web侧，H5链路上加载性能和体验方向上的一个指标

   - 4G/WIFI下，1秒完全完成页面加载，包括首屏资源，可看亦可用;
   - 3G下1秒完成首包的返回 ;
   - 2G下1秒完成建连。

3. Partition Torlerance 分区容错性

   分布式网络中部分网络不可用，但系统依然正常对外提供服务。

CP：不同空间中的数据，如果要求他们所有状态一致，则必然不在同一时间。

AP：不同空间中，如果要求同一时间都可以从任意的空间拿到数据，则必然数据的状态不一致。

CA：不同空间的数据，如果要求任意时间都可以从任意空间拿到状态一致的数据，则空间数必然为1.

zookeepr：**CP**，即任何时刻对zookeeper的访问请求能得到一致性的数据结果，同时系统对网络分割具备容错性，但是它不能保证每次服务的可用性。从实际情况来分析，在使用zookeeper获取服务列表时，如果zk正在选举或者zk集群中半数以上的机器不可用，那么将无法获取数据。所以说，zk不能保证服务可用性。

eureka：**AP**，eureka在设计时优先保证可用性，每一个节点都是平等的，一部分节点挂掉不会影响到正常节点的工作，不会出现类似zk的选举leader的过程，客户端发现向某个节点注册或连接失败，会自动切换到其他的节点，只要有一台eureka存在，就可以保证整个服务处在可用状态，只不过有可能这个服务上的信息并不是最新的信息。

## 对比网络传输格式

XML：文件大，性能低，结构严谨，可使用XSD校验

JSON：文件小，性能高，结构不严谨，可读性差

序列化：最小（二进制数据），性能最高，面向对象，无可读性

## 保证接口的幂等性

**单机**：打开页面时先将一个唯一编号保存在session域中，提交表单时判断session域中的数据是否一致。

**分布式**：生成一个全局唯一ID，插入redis中，每次请求先进redis判断

## 分布式Session

Tomcat + redis：在Tomcat配置文件中配置一下redis session

Spring Session + redis

## 分布式锁

**redis实现**：利用setnx的返回值特性，有值返回失败，无值返回成功。

redisson：一种基于redis的封装好的分布式锁。

**数据库实现**：使用一张锁表(id, lock_name)，其中lock_name唯一

每次执行业务前先查锁表中是否有这个lock_name，如有则失败，没有则插入锁表并执行业务，并删除锁。

**zookeeper实现**：有序临时节点 + watch监听

为每一个执行的线程创建一个有序的临时节点，为了确保有序性，在创建完节点，会再获取全部节点，会再获取全部节点，再重新进行一次排序，排序过程中，每个线程要判断自己的节点的序号是否是最小的。

如果是最小的，将会获取到锁，执行相关操作，释放锁

如果不是最小的，会监听到它的前一个节点，当它前一个节点被删除时，它就会获取锁，一次类推

## 分布式事务

### 2PC

1. 事务管理器询问每一个服务是否可以提交，任何一个服务拒绝，事务管理器全部回滚。

2. 每个服务都响应可以后执行。

**实现方式**：Spring + JTA（XA 协议的 Java 实现）

协调者有超时机制，如果在一定的时间内未收到客户端的消息默认失败

缺点：同步阻塞，单点故障

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210106220851600.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEwMzAyNg==,size_16,color_FFFFFF,t_70)

### 3PC

> 在两阶段上增加了：客户端超时机制和预提交机制

1. CanCommit：询问
2. preCommit：预提交
3. doCommit：提交

假如有任何一个 Conhort 向 Coordinator 发送了 No 响应，或者等待超时之后，Coordinator 都没有接收到 Cohort 的响应，那么就中断事务

发送中断请求：Coordinator 向所有 Cohort 发送 abort 请求

中断事务：Corhort 收到来自 Coordinator 的 abort 请求（或超时还未收到请求），执行事务中断

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210124101902970.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEwMzAyNg==,size_16,color_FFFFFF,t_70)

### TCC

1. Try：预先占有资源

2. Confirm：确认提交实际操作资源

3. Cancel：取消占有，即把那些执行成功的回滚。

**使用场景**：对分布式事务一致性要求高，如跟钱相关的。

**缺点**：重耦合，需要手写补偿逻辑。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210106221025144.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEwMzAyNg==,size_16,color_FFFFFF,t_70)

### Seata

http://seata.io/zh-cn/docs/overview/what-is-seata.html

> TC：Transaction Coordinator
>
> TM：Transaction Manager
>
> RM：Resorce Manager
>
> TM is a kind of RM.
>
> 本地锁：锁一个分布式事务中的一个服务
>
> 全局锁：锁一个分布式事务中的所有服务
>
> Seata 支持的事务模式：AT, TCC, SAGA, XA

**流程**

**分布式事务1**

获取本地锁 > 执行 SQL > <font color="red">获取全局锁 </font>> 提交本地事务 > 释放本地锁 > 全局提交 > 释放全局锁

**分布式事务2**

获取本地锁 > 执行 SQL > <font color="red">重复获取全局锁（有超时时间） </font>> 提交本地事务 > 释放本地锁 > 全局提交 > 释放全局锁

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210124195830118.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEwMzAyNg==,size_16,color_FFFFFF,t_70)

### 本地消息表

> 本地事务 + 定时任务 + 消息队列 + 事件表

1. A系统先插入数据到业务表，再插入消息表，再插入到MQ。

2. B系统接收到消息，先插入数据到消息表，再插入业务表，如果消息已被处理就回滚，防止重复消费。
3. B系统执行成功后，更新本地消息表以及 A 系统消息表的状态。
4. B系统执行失败，就不会更新，A系统会定时扫描本地消息表，如果有未处理消息，会再次发送到MQ。

**缺陷**：能保证事务的一致性，但是时效性太差

事件表字段

```sql
event_type: 事件类型
event_process: 事件环节 (new published processed)
event_content: 事件内容，保存事件发生时需要传递的数据
```

流程图

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210124102926916.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEwMzAyNg==,size_16,color_FFFFFF,t_70)

### 可靠消息服务 - 最大努力通知

1. A系统本地事务执行完之后，发送个消息到 MQ。
2. 最大努力通知服务会消费 MQ 然后写入数据库中记录下来，再调用B系统的接口。
3. 若系统 B 执行失败，最大努力通知服务会定时尝试重新调用B系统，反复N次，最后还是不行就放弃。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210124172031233.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEwMzAyNg==,size_16,color_FFFFFF,t_70)

## Dubbo

**RPC协议**

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210106221257207.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEwMzAyNg==,size_16,color_FFFFFF,t_70)

| 协议名称   | 实现描述                                     | 连接                                     | 使用场景                                                     |
| ---------- | -------------------------------------------- | ---------------------------------------- | ------------------------------------------------------------ |
| dubbo      | netty                                        | 单一长连接和NIO异步传输                  | 小数据量大并发的服务调用，消费者比提供者多，不适合传送大数据量的服务，比如文件、传视频 |
| rmi        | 采用JRM 作为通讯协议                         | 多连接，短连接，TCP/IP，BIO              | 可传文件，不支持防火墙穿透                                   |
| hessian    | hessian二进制序列化                          | 多连接，短连接，传输协议：HTTP，同步传输 | 提供者比消费者多 ，可传文件，跨语言传输                      |
| http       | 表单序列化                                   | 多连接，短连接，HTTP，同步传输           | 提供者大于消费者，数据包混合                                 |
| webservice | SOAP文件序列化                               | 多连接，短连接，HTTP，同步传输           | 系统集成，跨语言调用                                         |
| thrift     | 与thrift RPC实现集成，并在基础上修改了报文头 | 长连接、NIO异步传输                      |                                                              |

## Zookeeper

概念：一致有头数据树，解决分布式系统数据一致性问题。

**三种角色**

Leader：负责处理集群的写请求，并发起投票，只有超过半数的节点同意后才会提交该请求。

Follower：处理读请求，相应结果，转发写请求给leader，在选举leader时参与投票。

Observer：没有投票权的follower，协助follower处理读请求。

**应用场景**：配置中心、负载均衡、命名服务、DNS服务、集群管理

**zookeeper缺陷**：master宕机后剩余节点会重新进行leader选举，但选举时间太长（30-120s），且选举期间整个集群都是不可用的，这就导致了在选举期间注册服务瘫痪。

## 秒杀系统设计

> 越早拦截，成本越低，吞吐量越大

### 判断活动开始

Redis中存储一个ttl为抢购开始时间的key，各个服务器通过校验key是否过期，来判别活动开始。

### 开始抢购

客户端：按钮置灰，防止重复点击

网关：通过 Nginx-lua 直接查询 redis，发现超量过多时直接返回用户 "已售空"

服务端：

1. 基于 UserId 去重，防止刷单
2. 缓存预热，将要查询的热点数据先放入redis中，防止大量请求直接打到DB
3. 维护一个请求计数，只通过比实际库存量稍大的请求到MQ里，其余请求响应 "已售空"
4. 数据库更新采用乐观锁，防止超卖

服务端伪代码

```java
/* MQ 生产者 */
if (!redis.exists(userId)){				// 过滤重复用户
    redis.set(userId, "1");
}
if(redis.decr(goodsId) < 0){
    return "已售空";
} else {
    mq.convertAndSet(userId, goodsId);	// 将符合条件的用户放到 MQ 消费
}

/* MQ 消费者 */
update my_goods set stock = stock - 1, version = version + 1 where id = 1 and version = 0;
```


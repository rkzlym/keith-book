# IO模型

## 基本概念说明

**用户空间和内核空间**

操作系统的核心是内核，独立于普通的应用程序，<font color=blue>可以访问受保护的内存空间，也有访问底层硬件设备的所有权限</font>。为了保证用户进程不能直接操作内核，保证内核的安全，操心系统将虚拟空间划分为两部分：内核空间，用户空间。

**进程切换**

为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。<font color=blue>进程切换很消耗资源</font>。

**进程阻塞**

正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得CPU），才可能将其转为阻塞状态。<font color=blue>当进程进入阻塞状态，是不占用CPU资源的</font>。

**文件描述符**

是一个用于表述指向文件的引用的抽象化概念。形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。

**缓存IO**

缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。

## BIO

> 阻塞

同步阻塞，等待读写命令时，线程一直处于等待状态，即每次连接占用一个线程。

**流程**

1. 客户端进入时阻塞，等待服务端响应
2. 服务端成功响应后返回给客户端，客户端收到请求释放连接

**弊端**

用户需要等待read将socket中的数据读取到buffer后，才继续处理接收的数据。整个IO请求的过程中，用户线程是被阻塞的，这导致用户在发起IO请求时，不能做任何事情，对CPU的资源利用率不够。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210202170937685.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEwMzAyNg==,size_16,color_FFFFFF,t_70)

## NIO

> 轮询

同步非阻塞，使用selector轮询，遇到感兴趣的就处理(ACCEPT)，一个线程就可以完成一个服务端对应多个客户端。

**流程**

1. 客户端进入不阻塞，而是把每个进入的线程放入List
2. 对 List 里的线程进行遍历，如果有数据则直接返回

**弊端**

用户需要不断地调用read，尝试读取socket中的数据，直到读取成功后，才继续处理接收的数据。整个IO请求的过程中，虽然用户线程每次发起IO请求后可以立即返回，但是为了等到数据，仍需要不断地轮询、重复请求，消耗了大量的CPU的资源。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210202171203285.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEwMzAyNg==,size_16,color_FFFFFF,t_70)

## IO 多路复用

> Reactor 模型

有专门一个线程, 即 Acceptor 线程用于监听客户端的TCP连接请求

把1000个文件描述符传给select，内核因为线程调了select一次，所以把1000个文件描述符遍历一遍，返回几个文件描述符，这几个文件描述符去调读写数据的系统调用，需要自己完成数据写出，读入。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210109192025330.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEwMzAyNg==,size_16,color_FFFFFF,t_70)

## epoll

> 共享空间 mmap

上面的问题是传递数据的成本较高，粒度不够细

epoll可以在用户空间create epfd（epoll文件描述符），未来有一个连接进来就写给epoll文件描述符，epoll会准备一个共享空间mmap，mmap里维护了红黑树和链表，在mmap里的增删改是内核来完成的，查询两边都可以查。新进来的连接放入红黑树，用户空间调wait，等待事件，谁的数据到了就把这个节点往链表里放，wait从阻塞变为不阻塞取这个链表，把实际到达的这几个文件描述符取出到用户空间，然后用户空间再调读写方法。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210109192855527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEwMzAyNg==,size_16,color_FFFFFF,t_70)